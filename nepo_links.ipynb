{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3ad038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import wikipediaapi as wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cefc91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent and Relative strings to search for in wikipedia infobox\n",
    "parent_pattern = re.compile('Parent|Relative')\n",
    "\n",
    "# url base\n",
    "wiki_url_base = \"https://en.wikipedia.org\" # wikipedia\n",
    "imdb_base_url = \"https://www.imdb.com\"\n",
    "\n",
    "# wikipedia language setting\n",
    "wiki_wiki = wiki.Wikipedia('en')\n",
    "\n",
    "# imdb links\n",
    "# Most Popular TV Shows as determined by IMDb Users\n",
    "url_pop_tv = \"https://www.imdb.com/chart/tvmeter?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=470df400-70d9-4f35-bb05-8646a1195842&pf_rd_r=DK1W37ZH61RXZP184X95&pf_rd_s=right-4&pf_rd_t=15506&pf_rd_i=toptv&ref_=chttvtp_ql_5\"\n",
    "\n",
    "# Most Popular Movies as determined by IMDb Users\n",
    "url_pop_mov = \"https://www.imdb.com/chart/moviemeter?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=470df400-70d9-4f35-bb05-8646a1195842&pf_rd_r=408FM1WEANBKDGP51PET&pf_rd_s=right-4&pf_rd_t=15506&pf_rd_i=top&ref_=chttp_ql_2\"\n",
    "\n",
    "# Top Box Office (US) - updated weekly\n",
    "url_box_office = \"https://www.imdb.com/chart/boxoffice?pf_rd_m=A2FGELUUNOQJNL&pf_rd_p=470df400-70d9-4f35-bb05-8646a1195842&pf_rd_r=KK2KJN75YN0PGF8Q79QK&pf_rd_s=right-4&pf_rd_t=15506&pf_rd_i=moviemeter&ref_=chtmvm_ql_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4316cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given name of person, outputs parent links or false if none\n",
    "def wiki_scrape(name):\n",
    "    subject = name.replace(\" \", \"_\")\n",
    "    url = wiki_url_base + \"/wiki/\" + subject\n",
    "\n",
    "    # check if wiki page exists\n",
    "    wiki_page = wiki_wiki.page(subject)\n",
    "    does_page_exist = wiki_page.exists()\n",
    "\n",
    "    if does_page_exist is False :\n",
    "        return True # no wiki page -> not famous enough, not a nepo baby\n",
    "    \n",
    "    elif does_page_exist is True : \n",
    "        data = requests.get(url).text\n",
    "        soup = BeautifulSoup(data,'html.parser') # full page\n",
    "        \n",
    "        # check if infobox exists\n",
    "        infobox = soup.find(\"table\",{\"class\":\"infobox biography vcard\"}) # infobox\n",
    "        \n",
    "        if infobox is None :\n",
    "            return True # no infobox on wiki page -> not a nepo baby\n",
    "        else :\n",
    "            does_parent_field_exist = bool(re.search(\"Parent|Relative\", infobox.text)) # see if Parent or Relative field is listed in infobox\n",
    "        \n",
    "            if does_parent_field_exist is False :\n",
    "                return True # parent field not listed in infobox -> not a nepo baby\n",
    "            elif does_parent_field_exist is True :\n",
    "                parent_field = soup.find('th', string=parent_pattern).parent\n",
    "                parent_a_tags = parent_field.find_all('a')\n",
    "                if len(parent_a_tags) == 0 :\n",
    "                    return True # parents listed in infobox but not linked -> not a nepo baby\n",
    "                else : # nepo baby!\n",
    "                    parent_wiki_list = []\n",
    "                    for link in parent_field.find_all('a'):\n",
    "                        parent_wiki = link.get('href')\n",
    "                        parent_wiki_link = wiki_url_base + parent_wiki\n",
    "                        parent_wiki_list.append(parent_wiki_link)\n",
    "                        \n",
    "                        parent_wiki_list[:] = [x for x in parent_wiki_list if \"cite_note\" not in x] # cited entries are in <a href> tags so remove those links here\n",
    "    \n",
    "                return parent_wiki_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49d8c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_nepo(df):\n",
    "    df = df[['title', 'nepos']]\n",
    "    df = df[df.nepos != True].groupby('title').count() / df.groupby('title').count()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f87d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb_list_links(url):\n",
    "    page_data = requests.get(url).text\n",
    "    soup = BeautifulSoup(page_data,'html.parser')\n",
    "    \n",
    "    media_table = soup.find(\"table\", {\"class\":\"chart full-width\"}) \n",
    "    \n",
    "    all_links = []\n",
    "    \n",
    "    # media is linked twice in each row -- specify td (column) = 2 to avoid dupes\n",
    "    for tag in media_table.select(\"a\"):\n",
    "        all_links.append(imdb_base_url + tag[\"href\"])\n",
    "\n",
    "    # media is linked twice in each row \n",
    "    # tried select(\"td:nth-of-type(2) a\") in the for loop above but that didn't work for all imdb links\n",
    "    df = pd.DataFrame(columns=[\"link\"], data = all_links).drop_duplicates()\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d62eda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_cast(url):\n",
    "    page_data =  requests.get(url).text\n",
    "    soup = BeautifulSoup(page_data,'html.parser')\n",
    "    \n",
    "    title = soup.find(\"h1\")\n",
    "    title = title.text\n",
    "    \n",
    "    # grab top cast section of imdb page\n",
    "    top_cast = soup.find_all(\"a\", {\"class\": \"sc-36c36dd0-1 QSQgP\"})\n",
    "    \n",
    "    cast_list = []\n",
    "    \n",
    "    # get cast names\n",
    "    for a in top_cast:\n",
    "        cast_list.append(str(a.string))\n",
    "        \n",
    "    return[title, cast_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af3d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_whole_shebang(url):\n",
    "    df = get_imdb_list_links(url)\n",
    "    df['imdb_info'] = df.apply(lambda row : top_cast(row['link']), axis=1)\n",
    "    df = pd.DataFrame(df[\"imdb_info\"].to_list(), columns=['title', 'cast'])\n",
    "    df = df.explode('cast').reset_index(drop=True)\n",
    "    df.loc[:,\"nepos\"] = df.apply(lambda row : wiki_scrape(row['cast']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d31ebce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_tv_df = imdb_whole_shebang(url_pop_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "453470df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_mov_df = imdb_whole_shebang(url_pop_mov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb85e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_office_df = imdb_whole_shebang(url_box_office)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
