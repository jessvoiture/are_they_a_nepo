{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3ad038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import wikipediaapi as wiki\n",
    "import spotipy\n",
    "import billboard\n",
    "from datetime import datetime\n",
    "import imdb\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470498f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "###     CONSTANTS      ###\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cefc91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMdb API key\n",
    "api_key = \"k_11m5rq35\"\n",
    "\n",
    "# Parent and Relative strings to search for in wikipedia infobox\n",
    "parent_pattern = re.compile('Parent')\n",
    "relative_pattern = re.compile('Relative')\n",
    "\n",
    "not_nepo_relationships = 'son|daughter|nephew|niece|grandson|grand-daughter|grandnephew|grandniece|in-law|stepson|stepdaughter'\n",
    "\n",
    "# url base\n",
    "wiki_url_base = \"https://en.wikipedia.org\" # wikipedia\n",
    "imdb_base_url = 'https://www.imdb.com/title/'\n",
    "\n",
    "# wikipedia language setting\n",
    "wiki_wiki = wiki.Wikipedia('en')\n",
    "\n",
    "ia = imdb.Cinemagoer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97583c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_250_mov_url = \"https://imdb-api.com/en/API/Top250Movies/\"\n",
    "top_250_tv_url = \"https://imdb-api.com/en/API/Top250TVs/\"\n",
    "pop_mov_url = \"https://imdb-api.com/en/API/MostPopularMovies/\"\n",
    "pop_tv_url = \"https://imdb-api.com/en/API/MostPopularTVs/\"\n",
    "box_office_all_time_url = \"https://imdb-api.com/en/API/BoxOfficeAllTime/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89bf1723",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "###     FUNCTIONS      ###\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe760bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb_lists(url, imdb_api_key):\n",
    "    data = requests.get(url + imdb_api_key).json()\n",
    "    list_of_titles = data['items']\n",
    "    df = pd.DataFrame(list_of_titles)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d62eda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_specs(title_id):\n",
    "    page_data =  requests.get(imdb_base_url+title_id).text\n",
    "    soup = BeautifulSoup(page_data,'html.parser')\n",
    "    \n",
    "    # CAST\n",
    "    # grab top cast section of imdb page\n",
    "    top_cast = soup.find_all(\"a\", {\"class\": \"sc-36c36dd0-1 QSQgP\"})\n",
    "    \n",
    "    cast_list = []\n",
    "    \n",
    "    # get cast names\n",
    "    for a in top_cast:\n",
    "        cast_list.append(str(a.string))\n",
    "        \n",
    "    return cast_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4316cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given name of person, outputs parent links or false if none\n",
    "def wiki_scrape(name):\n",
    "    subject = name.replace(\" \", \"_\")\n",
    "    url = wiki_url_base + \"/wiki/\" + subject\n",
    "\n",
    "    # check if wiki page exists\n",
    "    wiki_page = wiki_wiki.page(subject)\n",
    "    does_page_exist = wiki_page.exists()\n",
    "\n",
    "    if does_page_exist is False :\n",
    "        return False # no wiki page -> not famous enough, not a nepo baby\n",
    "    \n",
    "    elif does_page_exist is True : \n",
    "        data = requests.get(url).text\n",
    "        soup = BeautifulSoup(data,'html.parser') # full page\n",
    "        \n",
    "        # check if infobox exists\n",
    "        infobox = soup.find(\"table\",{\"class\":\"infobox biography vcard\"}) # infobox\n",
    "        \n",
    "        if infobox is None :\n",
    "            return False # no infobox on wiki page -> not a nepo baby\n",
    "        \n",
    "        else :\n",
    "            does_parent_or_rel_field_exist = bool(re.search(\"Parent|Relative\", infobox.text)) # see if Parent or Relative field is listed in infobox\n",
    "        \n",
    "            if does_parent_or_rel_field_exist is False :\n",
    "                return False # parent field not listed in infobox -> not a nepo baby\n",
    "            \n",
    "            elif does_parent_or_rel_field_exist is True :\n",
    "                \n",
    "                try :\n",
    "                    does_parent_field_exist = bool(re.search(\"Parent\", infobox.text))\n",
    "                    does_relative_field_exist = bool(re.search(\"Relative\", infobox.text))\n",
    "                    \n",
    "                    if does_parent_field_exist is True :   \n",
    "                        parent_field = soup.find('th', string=parent_pattern).parent\n",
    "                        parent_a_tags = parent_field.find_all('a')\n",
    "                        \n",
    "                        if len(parent_a_tags) == 0 :\n",
    "                            return False # parents listed in infobox but not linked -> not a nepo baby\n",
    "                        \n",
    "                        else : # nepo baby!\n",
    "                            parent_wiki_list = []\n",
    "                            \n",
    "                            for link in parent_field.find_all('a'):\n",
    "                                parent_wiki = link.get('href')\n",
    "                                parent_wiki_link = wiki_url_base + parent_wiki\n",
    "                                parent_wiki_list.append(parent_wiki_link)\n",
    "\n",
    "                                parent_wiki_list[:] = [x for x in parent_wiki_list if \"cite_note\" not in x] # cited entries are in <a href> tags so remove those links here\n",
    "                                \n",
    "                        return parent_wiki_list\n",
    "\n",
    "                    elif does_relative_field_exist is True : \n",
    "                        relative_field = soup.find('th', string=relative_pattern).parent\n",
    "                        relative_td_tags = relative_field.find_all('td')\n",
    "\n",
    "                        list_of_relatives = re.split('</li>|<br/>', str(relative_td_tags))\n",
    "\n",
    "                        parent_wiki_list = []\n",
    "\n",
    "                        if len(relative_field.find_all('a')) == 0 :\n",
    "                            pass\n",
    "                        else :\n",
    "                            for i in range(len(list_of_relatives)):\n",
    "                                if bool(re.search(not_nepo_relationships, list_of_relatives[i])) is True:\n",
    "                                    pass\n",
    "\n",
    "                                else :\n",
    "                                    try :\n",
    "                                        href_match = re.search(r'href=\\\"(.*)\\\" title=', list_of_relatives[i])\n",
    "                                        parent_wiki = href_match.group(1)\n",
    "                                        parent_wiki_link = wiki_url_base + parent_wiki\n",
    "                                        parent_wiki_list.append(parent_wiki_link)\n",
    "\n",
    "                                    except AttributeError :\n",
    "                                        pass  \n",
    "                            \n",
    "                            if len(parent_wiki_list) == 0 :\n",
    "                                return False\n",
    "                            else :\n",
    "                                return parent_wiki_list\n",
    "                \n",
    "                except AttributeError :\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e00db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_nepo(df, group_col):\n",
    "    df = df[[group_col, 'nepos']]\n",
    "    df = df[df.nepos != False].groupby(group_col).count() / df.groupby(group_col).count() * 100\n",
    "    df = df.rename(columns={'nepos': 'pct_nepo'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2af3d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_whole_shebang(url):\n",
    "    \n",
    "    df = get_imdb_lists(url, api_key) # get list of titles from imdb\n",
    "    df['cast'] = df.apply(lambda row : title_specs(row['id']), axis=1) # get top cast from imdb\n",
    "    df = df.explode('cast').reset_index(drop=True) # expand list of cast into rows\n",
    "    df.loc[:,\"nepos\"] = df.apply(lambda row : wiki_scrape(row['cast']), axis=1) # check if each cast member is a nepo\n",
    "    df_pct_nepo = pct_nepo(df, \"fullTitle\") # calculate pct nepo by title\n",
    "    df = df.merge(df_pct_nepo, on=['fullTitle'], how=\"left\") # merge data frames\n",
    "    \n",
    "    csv_name_match = re.search('API/(.*)/', url)\n",
    "    csv_name = csv_name_match.group(1)\n",
    "    \n",
    "    df.to_csv(csv_name + \".csv\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907618fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "###   GET IMDB DATA    ###\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03a059be",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_250_mov = imdb_whole_shebang(top_250_mov_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f37a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_250_tv = imdb_whole_shebang(top_250_tv_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b9ed8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_mov = imdb_whole_shebang(pop_mov_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1024ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_tv = imdb_whole_shebang(pop_tv_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab2058",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### GET BILLBOARD DATA ###\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# charts start in 2016 and go up to most recent full year\n",
    "years_for_billboard_charts = list(range(2016, current_year-1))\n",
    "categories_for_billboard_charts = ['top-artists', 'top-rock-artists', 'top-r&b-artists', \n",
    "                                   'top-dance-electronic-artists', 'top-latin-artists']\n",
    "\n",
    "artists = []\n",
    "\n",
    "for yr in years_for_billboard_charts :\n",
    "    for category in categories_for_billboard_charts :\n",
    "        for i in billboard.ChartData(category, year=yr):\n",
    "            artists.append({'year': yr,\n",
    "                            'chart': category,\n",
    "                            'artist': i.artist})\n",
    "            \n",
    "artist_df = pd.DataFrame(artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67eb28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_df[\"nepos\"] = artist_df.apply(lambda row : wiki_scrape(row['artist']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_pct = pct_nepo(artist_df, 'Chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "###     USER INPUT     ###\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_input = input(\"name: \")\n",
    "person_cnxn = wiki_scrape(person_input)\n",
    "\n",
    "if person_cnxn is False :\n",
    "    print(f\"{person_input} is not a nepo baby\")\n",
    "else :\n",
    "    print(f'{person_input} is a nepo baby. Here are the connection links: {person_cnxn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084593c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_search_base = \"https://imdb-api.com/en/API/SearchTitle/k_11m5rq35/\"\n",
    "movie_input = input(\"movie or tv show: \")\n",
    "\n",
    "data = requests.get(url_search_base + movie_input).json()\n",
    "data['results']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aeb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_input = input(\"movie or tv show: \")\n",
    "movie_choices = ia.search_movie(movie_input)\n",
    "\n",
    "movie_choices_list = []\n",
    "\n",
    "# only showing 5 possible options\n",
    "for i in range(5) :\n",
    "    \n",
    "    index = [1, 2, 3, 4, 5]\n",
    "    rank = index[i]\n",
    "    \n",
    "    movie_title = movie_choices[i]['title']\n",
    "    movie_id = movie_choices[i].movieID\n",
    "    \n",
    "    movie = ia.get_movie(movie_id)\n",
    "    \n",
    "    short_cast_list = []\n",
    "    try :\n",
    "        cast = movie['cast']\n",
    "        for i in range(3):\n",
    "            short_cast_list.append(cast[i]['name'])\n",
    "    \n",
    "        formatted_cast_list = f\"starring {short_cast_list[0]}, {short_cast_list[1]}, and {short_cast_list[2]}\"\n",
    "    except KeyError:\n",
    "        formatted_cast_list = \"\"\n",
    "    except IndexError:\n",
    "        formatted_cast_list = \"\"\n",
    "        \n",
    "    try :\n",
    "        year = movie['year']\n",
    "        year = f\" ({year})\"\n",
    "    except KeyError:\n",
    "        year = \"\"\n",
    "    print(f\"{rank}: {movie_title}{year} {formatted_cast_list}\")\n",
    "\n",
    "movie_choice_input = input(f'people are not very creative with titles so there are a few titles - which number is the correct one?: ')\n",
    "\n",
    "movie_id = movie_choices[int(movie_choice_input)-1].movieID\n",
    "movie_url = imdb_base_url + \"tt\" + movie_id\n",
    "\n",
    "movie_full_cast = title_specs(\"tt\"+movie_id)\n",
    "\n",
    "df = pd.DataFrame([movie_full_cast], columns=['title', 'cast', 'imdb_rating'])\n",
    "df = df.explode('cast').reset_index(drop=True)\n",
    "df.loc[:,\"nepos\"] = df.apply(lambda row : wiki_scrape(row['cast']), axis=1)\n",
    "title_pct_nepo = pct_nepo(df, 'title')\n",
    "pct_nepo_str = title_pct_nepo['pct_nepo'][0]\n",
    "\n",
    "nepo_rows = df[df['nepos'] != False]\n",
    "\n",
    "print(f'{movie_input} is {pct_nepo_str}% full of nepotism babies. The following people are the culprits: {nepo_rows}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "###  IMDB RAND SAMPLE  ###\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f161fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_titles_per_pg_str = \"250\"\n",
    "number_of_titles_per_pg_int = 250\n",
    "number_of_titles_to_select = 250\n",
    "\n",
    "url_all = f\"https://www.imdb.com/search/title/?title_type=feature,tv_movie,tv_series,tv_miniseries&countries=us&count={number_of_titles_per_pg_str}&view=simple&sort=alpha,asc\"\n",
    "imdb_search_url = f\"https://imdb-api.com/API/AdvancedSearch/{api_key}/?title_type=feature,tv_movie,tv_series,tv_miniseries&countries=us&count={number_of_titles_per_pg_str}&sort=alpha,asc&start=\"\n",
    "\n",
    "data = requests.get(url_all).text\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "    \n",
    "# get total number of titles\n",
    "totals_tag = soup.find_all(\"div\", {\"class\" : \"desc\"})[0].find('span').text\n",
    "totals_tag = re.search('of (.*) titles', totals_tag)  \n",
    "total_ct_titles = int(totals_tag.group(1).replace(',', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab90fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly generate 250 titles (same numbers as in pop/top mov/tv lists)\n",
    "random_title_numbers = [random.randint(0, total_ct_titles) for iter in range(number_of_titles_to_select)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf724c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.DataFrame()\n",
    "\n",
    "for i in range(number_of_titles_to_select) :\n",
    "    \n",
    "    # each api request only pulls one page of data (250 titles)\n",
    "    # determine which page title is on and the index of that title on that page\n",
    "    pg_start = math.ceil(random_title_numbers[i] / number_of_titles_per_pg_int) * number_of_titles_per_pg_int + 1 # plus one because pages start on 1-, 251-, 501-, etc titles\n",
    "    num_title_on_pg = random_title_numbers[i] % number_of_titles_per_pg_int\n",
    "    \n",
    "    url_for_pg_num = imdb_search_url + str(pg_start)\n",
    "    \n",
    "    data = requests.get(url_for_pg_num).json()\n",
    "    data = data['results']\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    title = df.iloc[num_title_on_pg].transpose()\n",
    "    \n",
    "    titles = pd.concat([titles, title])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682dd5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "###       TESTING      ###\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b215ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get(\"https://en.wikipedia.org/wiki/Ted_Danson\").text\n",
    "soup = BeautifulSoup(data,'html.parser')\n",
    "\n",
    "infobox = soup.find(\"table\",{\"class\":\"infobox biography vcard\"})\n",
    "does_relative_field_exist = bool(re.search(\"Relative\", infobox.text))\n",
    "\n",
    "relative_field = soup.find('th', string=relative_pattern).parent\n",
    "relative_td_tags = relative_field.find_all('td')\n",
    "\n",
    "list_of_relatives = re.split('</li>|<br/>', str(relative_td_tags))\n",
    "\n",
    "parent_wiki_list = []\n",
    "\n",
    "if len(relative_field.find_all('a')) == 0 :\n",
    "    pass\n",
    "else :\n",
    "    for i in range(len(list_of_relatives)):\n",
    "        if bool(re.search(not_nepo_relationships, list_of_relatives[i])) is True:\n",
    "            pass\n",
    "        \n",
    "        else :\n",
    "            try :\n",
    "                href_match = re.search(r'href=\\\"(.*)\\\" title=', list_of_relatives[i])\n",
    "                parent_wiki = href_match.group(1)\n",
    "                parent_wiki_link = wiki_url_base + parent_wiki\n",
    "                parent_wiki_list.append(parent_wiki_link)\n",
    "                \n",
    "            except AttributeError :\n",
    "                pass\n",
    "            \n",
    "    if len(parent_wiki_list) == 0 :\n",
    "        return False\n",
    "    else :\n",
    "        return parent_wiki_list\n",
    "\n",
    "parent_wiki_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca6203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
